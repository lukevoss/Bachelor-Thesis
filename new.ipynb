{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\luke\\anaconda3\\envs\\torch_nish\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CI acc: 0.01, CI upper acc: 0.68, CI lower acc: 0.33\n",
      "Loss: 77.3148\n",
      "CI acc: 0.04, CI upper acc: 0.73, CI lower acc: 0.31\n",
      "Loss: 69.4747\n",
      "CI acc: 0.06, CI upper acc: 0.63, CI lower acc: 0.43\n",
      "Loss: 61.7484\n",
      "CI acc: 0.09, CI upper acc: 0.60, CI lower acc: 0.50\n",
      "Loss: 54.0158\n",
      "CI acc: 0.13, CI upper acc: 0.78, CI lower acc: 0.35\n",
      "Loss: 46.5165\n",
      "CI acc: 0.24, CI upper acc: 0.72, CI lower acc: 0.53\n",
      "Loss: 38.8187\n",
      "CI acc: 0.27, CI upper acc: 0.75, CI lower acc: 0.52\n",
      "Loss: 31.4939\n",
      "CI acc: 0.42, CI upper acc: 0.74, CI lower acc: 0.68\n",
      "Loss: 24.6717\n",
      "CI acc: 0.54, CI upper acc: 0.83, CI lower acc: 0.71\n",
      "Loss: 17.8546\n",
      "CI acc: 0.54, CI upper acc: 0.96, CI lower acc: 0.57\n",
      "Loss: 12.5065\n",
      "CI acc: 0.87, CI upper acc: 0.96, CI lower acc: 0.91\n",
      "Loss: 7.6664\n",
      "CI acc: 0.96, CI upper acc: 0.97, CI lower acc: 0.99\n",
      "Loss: 5.2111\n",
      "CI acc: 0.91, CI upper acc: 0.98, CI lower acc: 0.92\n",
      "Loss: 2.9341\n",
      "CI acc: 0.94, CI upper acc: 0.94, CI lower acc: 1.00\n",
      "Loss: 2.4700\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 0.99\n",
      "Loss: 2.3046\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 0.99\n",
      "Loss: 2.2420\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.8346\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.2975\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.8127\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.1538\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.1859\n",
      "CI acc: 0.96, CI upper acc: 0.99, CI lower acc: 0.97\n",
      "Loss: 1.6479\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.1440\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.2174\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.5157\n",
      "CI acc: 0.97, CI upper acc: 0.99, CI lower acc: 0.98\n",
      "Loss: 1.5609\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 2.4541\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.6279\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.3987\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.0802\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.3508\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.4124\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.2613\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.8240\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.3974\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 0.99\n",
      "Loss: 1.5969\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.1064\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.5365\n",
      "CI acc: 0.97, CI upper acc: 0.98, CI lower acc: 0.98\n",
      "Loss: 1.3830\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.1551\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.2359\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.0710\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.2865\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 0.9644\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.2275\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.2075\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.7927\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 0.99\n",
      "Loss: 2.1152\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 2.0446\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.0058\n",
      "CI acc: 0.94, CI upper acc: 0.98, CI lower acc: 0.96\n",
      "Loss: 1.6972\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.2720\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.1779\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.5148\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.1269\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.1496\n",
      "CI acc: 0.97, CI upper acc: 0.98, CI lower acc: 0.98\n",
      "Loss: 1.2235\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.1756\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.2835\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.8021\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.2686\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.4371\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.3887\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.4645\n",
      "CI acc: 0.97, CI upper acc: 0.98, CI lower acc: 0.98\n",
      "Loss: 2.0248\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.2551\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.2490\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.3069\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.0736\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.3771\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 0.99\n",
      "Loss: 1.3213\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.6375\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.3153\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.4232\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 2.8269\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 0.99\n",
      "Loss: 1.4409\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.3501\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 0.99\n",
      "Loss: 1.4092\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.1281\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.4334\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.3131\n",
      "CI acc: 0.95, CI upper acc: 0.99, CI lower acc: 0.96\n",
      "Loss: 1.5578\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 0.99\n",
      "Loss: 1.2182\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 0.99\n",
      "Loss: 2.5770\n",
      "CI acc: 0.95, CI upper acc: 0.98, CI lower acc: 0.97\n",
      "Loss: 1.7095\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.4327\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.2956\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.1675\n",
      "CI acc: 0.96, CI upper acc: 0.98, CI lower acc: 0.98\n",
      "Loss: 2.2467\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.4454\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.3646\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.4343\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.4593\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.3899\n",
      "CI acc: 0.97, CI upper acc: 0.98, CI lower acc: 0.98\n",
      "Loss: 1.4309\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.5559\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 0.9959\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.6739\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.2664\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.4213\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.9565\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.1305\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.4250\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.2786\n",
      "CI acc: 0.97, CI upper acc: 0.97, CI lower acc: 1.00\n",
      "Loss: 2.5124\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.1364\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.8910\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.2793\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.7866\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.6450\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.1364\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.4330\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.3116\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.2870\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.4107\n",
      "CI acc: 0.95, CI upper acc: 0.98, CI lower acc: 0.97\n",
      "Loss: 1.8163\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.7199\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.5151\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.1892\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.1931\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.5814\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.5267\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.3299\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 2.6530\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.4186\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.4919\n",
      "CI acc: 0.98, CI upper acc: 0.98, CI lower acc: 1.00\n",
      "Loss: 1.4663\n",
      "CI acc: 0.97, CI upper acc: 0.97, CI lower acc: 1.00\n",
      "Loss: 2.3600\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16568\\85622554.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     76\u001b[0m                            \u001b[0mcriterion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m                            \u001b[0msample_nbr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m                            complexity_cost_weight=1/X_train.shape[0])\n\u001b[0m\u001b[0;32m     79\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\luke\\anaconda3\\envs\\torch_nish\\lib\\site-packages\\blitz\\utils\\variational_estimator.py\u001b[0m in \u001b[0;36msample_elbo\u001b[1;34m(self, inputs, labels, criterion, sample_nbr, complexity_cost_weight)\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_nbr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn_kl_divergence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mcomplexity_cost_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\luke\\anaconda3\\envs\\torch_nish\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16568\\85622554.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mx_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblinear1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[0mx_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblinear2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\luke\\anaconda3\\envs\\torch_nish\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\luke\\anaconda3\\envs\\torch_nish\\lib\\site-packages\\blitz\\modules\\linear_bayesian_layer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias_sampler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[0mb_log_posterior\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias_sampler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_posterior\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m             \u001b[0mb_log_prior\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias_prior_dist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_prior\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\luke\\anaconda3\\envs\\torch_nish\\lib\\site-packages\\blitz\\modules\\weight_sampler.py\u001b[0m in \u001b[0;36mlog_prior\u001b[1;34m(self, w)\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \"\"\"\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[0mprob_n1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdist1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdist2\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\luke\\anaconda3\\envs\\torch_nish\\lib\\site-packages\\torch\\distributions\\normal.py\u001b[0m in \u001b[0;36mlog_prob\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;31m# compute the variance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0mlog_scale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mReal\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlog_scale\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from blitz.modules import BayesianLinear\n",
    "from blitz.utils import variational_estimator\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_boston(return_X_y=True)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "y = StandardScaler().fit_transform(np.expand_dims(y, -1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=.25,\n",
    "                                                    random_state=42)\n",
    "\n",
    "\n",
    "X_train, y_train = torch.tensor(X_train).float(), torch.tensor(y_train).float()\n",
    "X_test, y_test = torch.tensor(X_test).float(), torch.tensor(y_test).float()\n",
    "\n",
    "\n",
    "@variational_estimator\n",
    "class BayesianRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        #self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.blinear1 = BayesianLinear(input_dim, 512)\n",
    "        self.blinear2 = BayesianLinear(512, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_ = self.blinear1(x)\n",
    "        x_ = F.relu(x_)\n",
    "        return self.blinear2(x_)\n",
    "\n",
    "\n",
    "def evaluate_regression(regressor,\n",
    "                        X,\n",
    "                        y,\n",
    "                        samples = 100,\n",
    "                        std_multiplier = 2):\n",
    "    preds = [regressor(X) for i in range(samples)]\n",
    "    preds = torch.stack(preds)\n",
    "    means = preds.mean(axis=0)\n",
    "    stds = preds.std(axis=0)\n",
    "    ci_upper = means + (std_multiplier * stds)\n",
    "    ci_lower = means - (std_multiplier * stds)\n",
    "    ic_acc = (ci_lower <= y) * (ci_upper >= y)\n",
    "    ic_acc = ic_acc.float().mean()\n",
    "    return ic_acc, (ci_upper >= y).float().mean(), (ci_lower <= y).float().mean()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "regressor = BayesianRegressor(13, 1).to(device)\n",
    "optimizer = optim.Adam(regressor.parameters(), lr=0.01)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "ds_train = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "dataloader_train = torch.utils.data.DataLoader(ds_train, batch_size=16, shuffle=True)\n",
    "\n",
    "ds_test = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "dataloader_test = torch.utils.data.DataLoader(ds_test, batch_size=16, shuffle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4172],\n",
       "        [ 1.0304],\n",
       "        [-0.8416],\n",
       "        [ 0.9869],\n",
       "        [-0.2104],\n",
       "        [-1.0049],\n",
       "        [ 0.4427],\n",
       "        [-1.1572],\n",
       "        [-0.2648],\n",
       "        [ 2.9895],\n",
       "        [ 0.4971]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, (datapoints, labels) in enumerate(dataloader_train):\n",
    "    datapoints\n",
    "labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 0\n",
    "for epoch in range(1000):\n",
    "    for i, (datapoints, labels) in enumerate(dataloader_train):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = regressor.sample_elbo(inputs=datapoints.to(device),\n",
    "                           labels=labels.to(device),\n",
    "                           criterion=criterion,\n",
    "                           sample_nbr=3,\n",
    "                           complexity_cost_weight=1/X_train.shape[0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        iteration += 1\n",
    "        if iteration%100==0:\n",
    "            ic_acc, under_ci_upper, over_ci_lower = evaluate_regression(regressor,\n",
    "                                                                        X_test.to(device),\n",
    "                                                                        y_test.to(device),\n",
    "                                                                        samples=25,\n",
    "                                                                        std_multiplier=3)\n",
    "            \n",
    "            print(\"CI acc: {:.2f}, CI upper acc: {:.2f}, CI lower acc: {:.2f}\".format(ic_acc, under_ci_upper, over_ci_lower))\n",
    "            print(\"Loss: {:.4f}\".format(loss))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.1 ('torch_nish')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8b3fb30ea21cc95a61844aee09cc38bd7c92febb3a25f4f4f21db830aea750fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
