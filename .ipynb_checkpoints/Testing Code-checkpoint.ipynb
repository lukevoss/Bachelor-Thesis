{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ba942a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.util import module_for_loader\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers\n",
    "\n",
    "NUM_POINTS = 1000\n",
    "x = np.linspace(-5, 2, NUM_POINTS)\n",
    "y_target = 4 * x * np.cos(np.pi * np.sin(x)) + 1\n",
    "y = y_target + np.random.randn(x.shape[0]) * 0.5\n",
    "\n",
    "\n",
    "def prior(kernel_size, bias_size, dtype = None):\n",
    "    n = kernel_size + bias_size # num of params\n",
    "    return Sequential([\n",
    "       tfpl.DistributionLambda(\n",
    "           lambda t: tfd.Normal(loc = tf.zeros(n), scale= 2 * tf.ones(n))\n",
    "       )                     \n",
    "  ])\n",
    "\n",
    "def posterior(kernel_size, bias_size, dtype=None):\n",
    "    n = kernel_size + bias_size\n",
    "    return tf.keras.Sequential([\n",
    "      tfp.layers.VariableLayer(2 * n, dtype=dtype),\n",
    "      tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n",
    "\n",
    "          tfd.Normal(loc=t[..., :n],\n",
    "                     scale= 1e-5 + 0.003 * tf.nn.softplus(t[..., n:])),\n",
    "          reinterpreted_batch_ndims=1)),\n",
    "    ])\n",
    "\n",
    "event_shape = 1\n",
    "model_non_linear = Sequential([\n",
    "    \n",
    "    tfpl.DenseVariational(input_shape = (1,),\n",
    "                          units = 128,\n",
    "                          make_prior_fn = prior,\n",
    "                          make_posterior_fn = posterior,\n",
    "                          kl_weight = 1 / x.shape[0], activation = tf.nn.silu),\n",
    "    \n",
    "    tfpl.DenseVariational(units = 64,\n",
    "                          make_prior_fn = prior,\n",
    "                          make_posterior_fn = posterior,\n",
    "                          kl_weight = 1 / x.shape[0], activation = tf.nn.silu),\n",
    "\n",
    "    tfpl.DenseVariational(units = 1,\n",
    "                          make_prior_fn = prior,\n",
    "                          make_posterior_fn = posterior,\n",
    "                          kl_weight = 1 / x.shape[0]),\n",
    "\n",
    "    Dense(units= tfpl.IndependentNormal.params_size(event_shape)),\n",
    "    tfpl.IndependentNormal(event_shape)\n",
    "])\n",
    "nll = lambda y, p_y: -p_y.log_prob(y)\n",
    "\n",
    "model_non_linear.compile(optimizer=tf.optimizers.Adam(learning_rate = 0.002),\n",
    "              loss=nll)\n",
    "model_non_linear.summary()\n",
    "model_non_linear.fit(x, y, epochs=5000)\n",
    "\n",
    "ensemble_size = 10\n",
    "\n",
    "# plt.figure(0)\n",
    "# plt.scatter(x, y, s = 30, alpha = 1, marker = \"o\", color = 'red', label = 'Data')\n",
    "# plt.plot(x,y_target, linestyle = 'dashed', color = 'black', linewidth = 3, label = 'Target function')\n",
    "# for i in range(ensemble_size):\n",
    "#     #define upper bounds and lower bound of epistemic uncertainty\n",
    "#     if i == 0:\n",
    "#         y_upper = model_non_linear(x)\n",
    "#         y_lower = y_upper\n",
    "#     else:\n",
    "#         y_upper = np.maximum(y_upper, model_non_linear(x))  \n",
    "#         y_lower = np.minimum(y_lower, model_non_linear(x))        \n",
    "# plt.fill_between(x, y_upper[:,0], y_lower[:,0], alpha = 0.6, color='royalblue', label='epistemic uncertainty')      \n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "plt.figure(0)\n",
    "plt.scatter(x, y, s = 30, alpha = 1, marker = \"o\", color = 'red', label = 'Data')\n",
    "plt.plot(x,y_target, linestyle = 'dashed', color = 'black', linewidth = 3, label = 'Target function')\n",
    "\n",
    "y_mean_list = np.zeros((NUM_POINTS, ensemble_size))\n",
    "y_upper_list = np.zeros((NUM_POINTS, ensemble_size))\n",
    "y_lower_list = np.zeros((NUM_POINTS, ensemble_size))\n",
    "for i in range(ensemble_size):\n",
    "    model_distribution = model_non_linear(x)\n",
    "    model_means = model_distribution.mean().numpy()\n",
    "    plt.plot(x,model_means, color='blue')\n",
    "    y_mean_list[:,i] = model_means[:,0]\n",
    "    #standard deviation\n",
    "    model_std = model_distribution.stddev().numpy()\n",
    "    y_std_upper = model_means + 2 * model_std\n",
    "    y_std_lower = model_means - 2 * model_std\n",
    "    y_upper_list[:,1] = y_std_upper[:,0]\n",
    "    y_lower_list[:,1] = y_std_lower[:,0]\n",
    "y_mean = np.mean(y_mean_list, axis=1)\n",
    "y_upper = np.mean(y_upper_list, axis=1)\n",
    "y_lower = np.mean(y_lower_list, axis=1)\n",
    "plt.plot(x, y_mean, color= 'cornflowerblue', alpha=0.8, linewidth = 3, label='learned model $\\mu$')\n",
    "plt.fill_between(x, y_upper, y_lower, alpha = 0.4, color='skyblue', label='Standard Deviation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
